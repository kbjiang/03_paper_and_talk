- https://youtu.be/t70Bl3w7bxY
- the timeline: Transformers, GPT, BERT, GPT-2, GPT-3
### GPT
1. decoder, objective function
2. fine tuning: 
	1. special token: *start, delim, extract*
	2. multiple obj func

### GPT-2
1. beaten by BERT on downstream tasks, so focus on zero-shot, not as useful/impactful
2. introduced prompt, and *why it works*

### GPT-3
1. few-shot learner; *no gradient updates or fine-tuning*
2. problem with fine-tuning as a metric: labeling, overfitting by chance
3. GPT few-shots cannot leverage too many task-specific examples. Can we design an external memory for it? https://youtu.be/t70Bl3w7bxY?t=3836
