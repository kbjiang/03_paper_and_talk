## **A brief summary of language model finetuning**
Tags: #LLM #finetune
Authors: Cameron R. Wolfe
Talk link: 
Post link: https://stackoverflow.blog/2024/10/31/a-brief-summary-of-language-model-finetuning
Related: 
### Why interesting
### Ideas/conclusions
- Most knowledge from an LLM comes from pretraining.
- We can perform fine tuning in the form of continued pretraining to expose the LLM to more (and new) data/knowledge.
- Alignment-focused objectives can be achieved via fine tuning (SFT) on small, high-quality datasets. We donâ€™t need tons of data to learn the style or format of output, only to learn new knowledge.

