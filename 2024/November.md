## **A brief summary of language model finetuning**
Tags: #LLM #finetune
Authors: Cameron R. Wolfe
Talk link: 
Post link: https://stackoverflow.blog/2024/10/31/a-brief-summary-of-language-model-finetuning
Related: 
### Why interesting
### Ideas/conclusions
- Most knowledge from an LLM comes from pretraining.
- We can perform fine tuning in the form of continued pretraining to expose the LLM to more (and new) data/knowledge.
- Alignment-focused objectives can be achieved via fine tuning (SFT) on small, high-quality datasets. We donâ€™t need tons of data to learn the style or format of output, only to learn new knowledge.

## **How Transformers Learn Causal Structure with Gradient Descent**^transformer-causal

Tags: #UnderstandTransformers 
Authors: Jason Lee
Talk link: https://youtu.be/xlWBsISnaRA
Post link: 
Related: 
### Why interesting
### Ideas/conclusions

### References
1. https://transformer-circuits.pub/2021/framework/index.html